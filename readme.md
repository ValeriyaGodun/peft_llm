# Дообучение ruT5-large с LoRA и Prompt Tuning

## Описание задачи

**Цель:** дообученить модель ruT5-large для решения задач генерации текста на русском языке. 

В данном проекте применяются два метода Parameter-Efficient Fine-Tuning (PEFT): Low-Rank Adaptation (LoRA) и Prompt Tuning. Также в ClearML логируются гипермараметры, сохраняется лучшая модель и отслеживаются метрики обучения и валидации.

## Данные

Модель обучается на датасете `lksy/ru_instruct_gpt4`, который содержит наборы инструкций и ответов к ним, сгенерированных с помощью GPT-4.

- **Обучающая выборка**: 12,931 примеров
- **Тестовая выборка**: 1,125 примеров

**Каждый пример содержит:**
- `instruction`: Текст инструкции для выполнения задачи
- `input`: Дополнительный контекст (может быть пустым)
- `output`: Ожидаемый ответ модели
- `full_output`: Расширенный ответ (может быть пустым)

```
{'instruction': 'Посоветуй, какие три вида спорта сделают лето интереснее и забавнее для 12-летнего ребенка.',
 'input': None,
 'output': 'Летом 12-летнему ребенку могут быть интересны следующие виды спорта: катание на роликах, которое тренирует координацию и баланс; плавание, укрепляющее мышцы и обеспечивающее охлаждение в жару; и фрисби, улучшающее ловкость, скорость и активное общение на свежем воздухе.',
 'full_output': '1. Плавание: Летом отлично провести время на пляже или в бассейне, занимаясь плаванием. Это не только интересное и веселое развлечение, но еще и полезное для здоровья и физического развития ребенка.\n\n2. Велосипедный спорт: Прогулки на велосипеде разнообразят летние дни, позволят 12-летнему ребенку активно проводить время на свежем воздухе и укреплять мышцы ног.\n\n3. Бадминтон: Это веселая и энергичная игра развивает координацию, внимание и гибкость, и при этом не требует сложного оборудования. Бадминтон можно играть как в парке, так и на личном дворе.'}
 ```

**Предобработка данных:**

Данные загружаются из TSV файлов и предобрабатываются функцией `preprocess`, которая:
- Объединяет инструкцию и входной текст
- Токенизирует тексты
- Обрезает последовательности до максимальной длины

## Архитектура решения

Дообучение модели строится на комбинации двух методов:

1. **LoRA (Low-Rank Adaptation)**: Добавляет адаптеры к линейным слоям query и value в механизме внимания. Параметры адаптеров обучаются, в то время как базовые веса модели остаются замороженными.

2. **Prompt Tuning**: Добавляет обучаемые промпт-эмбеддинги в начало входной последовательности. Эти эмбеддинги обучаются совместно с LoRA-адаптерами.

### Реализация

**LoRALayer**: Класс, который модифицирует линейный слой с помощью LoRA. Матрица А инициализируется через `Kaiming Uniform Initialization`, а В - нулями.

**PromptTuningEmbedding**: Класс, оборачивающий стандартные эмбеддинги токенов и добавляющий обучаемые промпт-эмбеддинги фиксированного размера в начало последовательности.

**PTDataCollator**: DataCollator, расширяющий DataCollatorForSeq2Seq для корректной обработки промпт-токенов в батчах. Он добавляет матрицу из единиц в начало маски внимания для Encoder'а, чтобы выровнить ее размер.

### Гиперпараметры модели

```
Базовая модель: ai-forever/ruT5-large
LoRA rank: 8
Размер промпта: 20 токенов
Максимальная длина входа: 100 токенов
Максимальная длина выхода: 200 токенов
```

### Параметры обучения

``` Batch size: 8
Gradient accumulation steps: 4
Learning rate: 3e-4
Weight decay: 0.01
Количество эпох: 10
Gradient checkpointing: Включен
Mixed precision: FP16 
```

### Статистика параметров

``` 
Всего параметров: 740,047,872
Обучаемых параметров: 2,379,776
Процент обучаемых параметров: 0.32%
```

### Генерация предсказаний

Для генерации предсказаний используется функция `generate_predictions`, которая:
- подготавливает входные данные с промпт-токенами
- выполняет генерацию с настраиваемыми параметрами (beam search, temperature, top-k, top-p)
- возвращает декодированные предсказания
  
### Метрики оценки

Модель оценивается на тестовой выборке с использованием следующих метрик:
- **BLEU** - метрика точности, основанная на n-граммах. Оценивает совпадение последовательностей токенов между предсказанием и эталоном.
- **ROUGE-1** - метрика полноты, измеряющая перекрытие отдельных слов между предсказанием и эталоном. Фокусируется на лексическом совпадении на уровне слов.
- **METEOR** - метрика, учитывающая не только точное совпадение слов, но и синонимы, стемминг и порядок слов.

## Результаты

### Примеры генерации

1. ```
    num_beams=4
    temperature=0.2
    top_k=30
    top_p=0.95
   ```
`input_text` = Придумайте название для книги на основе следующего описания сюжета.\n\nМолодой парень находит портал в параллельный мир, где его жизнь намного хуже, чем она была до этого.

`predictions` = "Портал в параллельный мир" - это роман, в котором молодой человек обнаруживает портал в параллельный мир, где его жизнь намного хуже, чем была до этого.

`input_text` = Придумай мотивирующую цитату. Спорт, усилия, результат.

`predictions` = Спорт - это труд, который приносит результат.

2. ```
    num_beams=1
    temperature=0.9
    top_k=50
    top_p=0.9
   ```
`input_text` = Что означает слово "землетрясение".

`predictions` = Землетрясение - это физическое явление, которое происходит, когда земля испытывает значительные разрушения и испытывает сильнейшие деформации. Поверхностные слои почвы размягчаются и разрушаются, что может привести к появлению больших волн и землетрясений, включая землетрясения, грады, землетрясения и пожары.

`input_text` = Придумай мотивирующую цитату, которая поднимет настроение для учебы.

`predictions` = Учитесь! Учитесь! Занимайтесь, развивайтесь! Учитесь, и все будет хорошо!

3. ```
    num_beams=4
    temperature=0.8
    top_k=50
    top_p=0.95
   ```
`input_text = Расскажи историю про медведя`

`predictions = Жил-был медвежонок по имени Иван. Однажды, когда Иван был на охоте, он встретил медведя по имени Иван. Иван встретил медведя и рассказал ему свою историю о том, как он спас медведя от смерти.`

`input_text = Расскажи историю о девочке. В истории должны быть:игры, подружка, счастливая история`

`predictions = Девочка Катя, маленькая девочка, жила в тихом и уютном доме на окраине города. Однажды она встретила девочку по имени Наташа, которая была красавицей и красавицей. Настал день, когда девочка стала знаменитой и стала известной актрисой. Подруга всегда была довольна, и они стали неразлучными друзьями.`

### Вывод

На тестовой выборке модель показала следующие результаты:

- **BLEU**: `0.0634` - низкое значение метрики точности указывает на ограниченное лексическое совпадение между сгенерированными и эталонными текстами
- **ROUGE-1**: `0.0915` - метрика полноты демонстрирует невысокий уровень перекрытия униграмм между предсказаниями и эталонами
- **METEOR**: `0.1972` - наилучший результат среди всех метрик, что свидетельствует о более высокой семантической близости генераций благодаря учету синонимов и морфологических вариаций

Наблюдается значительный разброс значений метрик. Преимущество METEOR можно объяснить его способностью учитывать семантические связи между словами, а не только точное лексическое совпадение. Однако значения всех метрик все-таки остаются низкими, что указывает на необходимость дальнейшей оптимизации модели. 

В качестве решения данной проблемы можно увеличить размер обучающей выборки, увеличить число эпох обучения с применением `early stopping` для предотвращения переобучения, также можно провести дополнительные эксперименты с различными значениями `lora_rank` и размером промпта.

В дополнение к этому можно рассмотреть возможность дополнения метрик человеческой оценкой для получения более точных результатов о работе модели. Однако данный подход требует значительных временных и финансовых ресурсов (привлечение разметчиков и организация всего процесса разметки), что делает его менее доступным в сравнении с автоматическими метриками.
