{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76c49182-07b3-4400-b358-7a152d0eb112",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:24:17.025337Z",
          "iopub.status.busy": "2026-02-09T09:24:17.025061Z",
          "iopub.status.idle": "2026-02-09T09:24:30.173794Z",
          "shell.execute_reply": "2026-02-09T09:24:30.172809Z",
          "shell.execute_reply.started": "2026-02-09T09:24:17.025303Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Установка зависимостей\n",
        "!pip install clearml\n",
        "!pip install evaluate -q\n",
        "!pip install nltk -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70db6d3f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:24:30.175807Z",
          "iopub.status.busy": "2026-02-09T09:24:30.175275Z",
          "iopub.status.idle": "2026-02-09T09:25:06.966093Z",
          "shell.execute_reply": "2026-02-09T09:25:06.965253Z",
          "shell.execute_reply.started": "2026-02-09T09:24:30.175764Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Импортируем необходимые библиотеки\n",
        "import math\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "from transformers.integrations import ClearMLCallback\n",
        "\n",
        "from clearml import Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23dd16bb-dbe2-481a-ac57-ad8e5a57f058",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:25:06.968147Z",
          "iopub.status.busy": "2026-02-09T09:25:06.967581Z",
          "iopub.status.idle": "2026-02-09T09:25:06.974980Z",
          "shell.execute_reply": "2026-02-09T09:25:06.974217Z",
          "shell.execute_reply.started": "2026-02-09T09:25:06.968115Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Настраиваем окружения ClearML \n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=''\n",
        "%env CLEARML_API_SECRET_KEY=''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52f10157-354c-4d22-8c24-db7a3c517135",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-08T13:51:59.289349Z",
          "iopub.status.busy": "2026-02-08T13:51:59.288933Z",
          "iopub.status.idle": "2026-02-08T13:52:02.778606Z",
          "shell.execute_reply": "2026-02-08T13:52:02.777088Z",
          "shell.execute_reply.started": "2026-02-08T13:51:59.289283Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Инициализируем эксперимент в ClearML\n",
        "task_lora_pt = Task.init(\n",
        "        project_name=\"PEFT\",\n",
        "        task_name=\"LoRA_PT_10epoch\",\n",
        "        task_type=\"training\"\n",
        ")\n",
        "task:Task\n",
        "\n",
        "logger = task_lora_pt.get_logger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e3a3b7a-4391-4aa0-a4b1-3513402bec5b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-08T13:52:02.782445Z",
          "iopub.status.busy": "2026-02-08T13:52:02.780324Z",
          "iopub.status.idle": "2026-02-08T13:52:04.197522Z",
          "shell.execute_reply": "2026-02-08T13:52:04.196906Z",
          "shell.execute_reply.started": "2026-02-08T13:52:02.782388Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Логируем основные гиперпараметры эксперимента\n",
        "task_lora_pt.connect({\n",
        "    \"model\": \"ruT5-large\",\n",
        "    \"method\": \"LoRA\",\n",
        "    \"lora_rank\": 8,\n",
        "    \"per_device_train_batch_size\": 8,\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"gradient_checkpointing\": False,\n",
        "    \"learning_rate\": 3e-4,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"num_train_epochs\": 10,\n",
        "    \"fp16\": True,\n",
        "    \"max_input_length\": 100,\n",
        "    \"max_output_length\": 200,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7879b6a3-1543-4575-a547-c210d219b6c2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:28:09.216870Z",
          "iopub.status.busy": "2026-02-09T09:28:09.216203Z",
          "iopub.status.idle": "2026-02-09T09:28:09.228235Z",
          "shell.execute_reply": "2026-02-09T09:28:09.227532Z",
          "shell.execute_reply.started": "2026-02-09T09:28:09.216833Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Фиксируем сиды для воспроизводимости экспериментов\n",
        "def fix_seeds(seed: int):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed) \n",
        "\n",
        "fix_seeds(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1dda4bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Выбираем устройство для обучения\n",
        "device = torch.device('cuda')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a84dbc5-3bd2-4ef0-b236-ac00115288e2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:25:06.978998Z",
          "iopub.status.busy": "2026-02-09T09:25:06.978736Z",
          "iopub.status.idle": "2026-02-09T09:25:07.821107Z",
          "shell.execute_reply": "2026-02-09T09:25:07.820558Z",
          "shell.execute_reply.started": "2026-02-09T09:25:06.978963Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Загружаем обучающий и тестовый датасеты \n",
        "data_files = {'train': '/kaggle/input/ru_instruct_gpt4_train.tsv', 'test': '/kaggle/input/ru_instruct_gpt4_test.tsv'}\n",
        "dataset = load_dataset('csv', data_files=data_files, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f81d8ba-af93-4e28-a796-692bc6196b21",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:25:40.885568Z",
          "iopub.status.busy": "2026-02-09T09:25:40.885227Z",
          "iopub.status.idle": "2026-02-09T09:25:40.895125Z",
          "shell.execute_reply": "2026-02-09T09:25:40.894460Z",
          "shell.execute_reply.started": "2026-02-09T09:25:40.885535Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "693fa428-6779-4c6a-a935-b788483aeafd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:25:44.170382Z",
          "iopub.status.busy": "2026-02-09T09:25:44.169624Z",
          "iopub.status.idle": "2026-02-09T09:25:57.328940Z",
          "shell.execute_reply": "2026-02-09T09:25:57.327692Z",
          "shell.execute_reply.started": "2026-02-09T09:25:44.170330Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Загружаем токенизатор и базовую модель ruT5-large\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruT5-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"ai-forever/ruT5-large\", device_map='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29eca2c3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:25:57.330973Z",
          "iopub.status.busy": "2026-02-09T09:25:57.330573Z",
          "iopub.status.idle": "2026-02-09T09:25:57.352174Z",
          "shell.execute_reply": "2026-02-09T09:25:57.345084Z",
          "shell.execute_reply.started": "2026-02-09T09:25:57.330925Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Функция предобработки данных - объединяет инструкцию и контекст, токенизирует тексты\n",
        "def preprocess(samples: dict) -> dict:\n",
        "    input_texts = [f'{instr}\\n\\n{inp}' for instr, inp in zip(samples['instruction'], samples['input'])]\n",
        "    \n",
        "    input_tokenized = tokenizer(\n",
        "        input_texts,\n",
        "        max_length=100,\n",
        "        truncation=True,\n",
        "        return_token_type_ids=False\n",
        "    )['input_ids']\n",
        "\n",
        "    output_tokenized = tokenizer(\n",
        "        samples['output'],\n",
        "        max_length=200,\n",
        "        truncation=True,\n",
        "        return_token_type_ids=False\n",
        "    )['input_ids']\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_tokenized,\n",
        "        'labels': output_tokenized\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae0acae7-0402-4b83-b85a-5a5292804446",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:25:57.357662Z",
          "iopub.status.busy": "2026-02-09T09:25:57.356948Z",
          "iopub.status.idle": "2026-02-09T09:26:06.677079Z",
          "shell.execute_reply": "2026-02-09T09:26:06.676318Z",
          "shell.execute_reply.started": "2026-02-09T09:25:57.357529Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Выполняем предобработку и удаляем неиспользуемое поле full_output\n",
        "tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=['full_output'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9c484b8-5fa8-41de-9cca-ad7b65a1985a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:26:06.678800Z",
          "iopub.status.busy": "2026-02-09T09:26:06.678507Z",
          "iopub.status.idle": "2026-02-09T09:26:10.198165Z",
          "shell.execute_reply": "2026-02-09T09:26:10.197336Z",
          "shell.execute_reply.started": "2026-02-09T09:26:06.678773Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Загружаем метрики качества генерации\n",
        "bleu_metric = load(\"bleu\")\n",
        "rouge_metric = load(\"rouge\")\n",
        "meteor_metric = load(\"meteor\")\n",
        "\n",
        "# Функция для вычисления метрик BLEU, ROUGE и METEOR для оценки качества генерации\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    \n",
        "    if len(predictions.shape) == 3:\n",
        "        predictions = np.argmax(predictions, axis=-1)\n",
        "    \n",
        "    predictions = np.array(predictions, dtype=np.int32)\n",
        "    \n",
        "    vocab_size = len(tokenizer)\n",
        "    predictions = np.clip(predictions, 0, vocab_size - 1)\n",
        "    \n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    \n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    labels = np.array(labels, dtype=np.int32)\n",
        "    labels = np.clip(labels, 0, vocab_size - 1)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    decoded_labels = [[label] for label in decoded_labels]\n",
        "    \n",
        "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    meteor_result = meteor_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    \n",
        "    return {\n",
        "        'bleu': bleu_result['bleu'],\n",
        "        'rouge1': rouge_result['rouge1'],\n",
        "        'rouge2': rouge_result['rouge2'],\n",
        "        'rougeL': rouge_result['rougeL'],\n",
        "        'meteor': meteor_result['meteor']\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "930cf273-95a4-4da8-9c3d-e4aae832de20",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-08T13:52:47.932453Z",
          "iopub.status.busy": "2026-02-08T13:52:47.931787Z",
          "iopub.status.idle": "2026-02-08T13:52:48.865432Z",
          "shell.execute_reply": "2026-02-08T13:52:48.864766Z",
          "shell.execute_reply.started": "2026-02-08T13:52:47.932421Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Базовая генерация до дообучения для сравнения\n",
        "\n",
        "lm_text='Придумайте название для книги на основе следующего описания сюжета.\\n\\nМолодой парень находит портал в параллельный мир, где его жизнь намного хуже, чем она была до этого.'\n",
        "\n",
        "input_ids = torch.tensor([tokenizer.encode(lm_text)]).to(device)\n",
        "\n",
        "outputs = model.generate(input_ids, eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "print(tokenizer.decode(outputs[0][1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "057a0348",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:26:10.199850Z",
          "iopub.status.busy": "2026-02-09T09:26:10.199138Z",
          "iopub.status.idle": "2026-02-09T09:26:19.425535Z",
          "shell.execute_reply": "2026-02-09T09:26:19.424567Z",
          "shell.execute_reply.started": "2026-02-09T09:26:10.199813Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# LoRA-адаптер для слоев внимания\n",
        "\n",
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, module: nn.Linear, rank: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.module = module\n",
        "        self.adapter_A = nn.Parameter(\n",
        "            torch.empty(module.in_features, rank, device=module.weight.device)\n",
        "        )\n",
        "        self.adapter_B = nn.Parameter(\n",
        "            torch.zeros(rank, module.out_features, device=module.weight.device)\n",
        "        )\n",
        "\n",
        "        nn.init.kaiming_uniform_(self.adapter_A, a=math.sqrt(5))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        output = self.module(hidden_states)\n",
        "        lora_delta = hidden_states @ self.adapter_A @ self.adapter_B\n",
        "        output += lora_delta\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0114d4be",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:26:19.427009Z",
          "iopub.status.busy": "2026-02-09T09:26:19.426600Z",
          "iopub.status.idle": "2026-02-09T09:26:19.439679Z",
          "shell.execute_reply": "2026-02-09T09:26:19.438961Z",
          "shell.execute_reply.started": "2026-02-09T09:26:19.426965Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Функция для добавления LoRA-адаптеров к механизму внимания модели\n",
        "\n",
        "def add_lora(model, lora_rank=8):\n",
        "    \n",
        "    device = next(iter(model.parameters())).device\n",
        "    \n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'q') and isinstance(module.q, nn.Linear):\n",
        "            module.q = LoRALayer(module.q, lora_rank).to(device)\n",
        "        \n",
        "        if hasattr(module, 'v') and isinstance(module.v, nn.Linear):\n",
        "            module.v = LoRALayer(module.v, lora_rank).to(device)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7852ccd1-b107-4f12-afb1-3a4b77da41a4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:26:19.456638Z",
          "iopub.status.busy": "2026-02-09T09:26:19.456253Z",
          "iopub.status.idle": "2026-02-09T09:26:19.470812Z",
          "shell.execute_reply": "2026-02-09T09:26:19.470023Z",
          "shell.execute_reply.started": "2026-02-09T09:26:19.456596Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Prompt Tuning - добавляет обучаемые промпт-эмбеддинги в начало последовательности\n",
        "class PromptTuningEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_tokens, prompt_size: int):\n",
        "        super().__init__()\n",
        "        if isinstance(embed_tokens, PromptTuningEmbedding):\n",
        "            self.embed_tokens = embed_tokens.embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        \n",
        "        self.prompt_size = prompt_size\n",
        "        if isinstance(self.embed_tokens, nn.Embedding):\n",
        "            embedding_dim = self.embed_tokens.weight.shape[-1]\n",
        "        else:\n",
        "            embedding_dim = getattr(self.embed_tokens, 'embedding_dim', None)\n",
        "            if embedding_dim is None:\n",
        "                embedding_dim = getattr(self.embed_tokens, 'output_dim', None)\n",
        "            if embedding_dim is None:\n",
        "                try:\n",
        "                    device = next(self.embed_tokens.parameters()).device\n",
        "                except (StopIteration, AttributeError):\n",
        "                    if hasattr(self.embed_tokens, 'weight'):\n",
        "                        device = self.embed_tokens.weight.device\n",
        "                    else:\n",
        "                        device = torch.device('cpu')\n",
        "                test_input = torch.tensor([[0]], device=device)\n",
        "                test_output = self.embed_tokens(test_input)\n",
        "                embedding_dim = test_output.shape[-1]\n",
        "        \n",
        "        self.learnable_prompts = nn.Parameter(\n",
        "            torch.randn(1, prompt_size, embedding_dim), requires_grad=True\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        if input_ids.shape[1] >= self.prompt_size:\n",
        "            real_input_ids = input_ids[:, self.prompt_size:]\n",
        "            inputs_embeds = self.embed_tokens(real_input_ids)\n",
        "        else:\n",
        "            inputs_embeds = self.embed_tokens(input_ids)\n",
        "\n",
        "        batch_size = len(input_ids)\n",
        "        prompt_embeds = (\n",
        "            self.learnable_prompts\n",
        "            .expand(batch_size, self.prompt_size, -1)\n",
        "        )\n",
        "        \n",
        "        embeds = torch.cat((prompt_embeds, inputs_embeds), dim=1)\n",
        "\n",
        "        return embeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c36152-2b38-4405-b793-2378e156339d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:26:19.442019Z",
          "iopub.status.busy": "2026-02-09T09:26:19.441388Z",
          "iopub.status.idle": "2026-02-09T09:26:19.455336Z",
          "shell.execute_reply": "2026-02-09T09:26:19.454568Z",
          "shell.execute_reply.started": "2026-02-09T09:26:19.441974Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# DataCollator для корректной обработки промпт-токенов в батчах\n",
        "\n",
        "class PTDataCollator(DataCollatorForSeq2Seq):\n",
        "    def __init__(self, prompt_size: int, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prompt_size = prompt_size\n",
        "\n",
        "    def __call__(self, features):\n",
        "        if features and len(features) > 0:\n",
        "            output = super().__call__(features)\n",
        "        else:\n",
        "            return None\n",
        "        \n",
        "        if output is None or 'attention_mask' not in output or 'input_ids' not in output:\n",
        "            return output\n",
        "        \n",
        "        attention_mask = output['attention_mask']\n",
        "        input_ids = output['input_ids']\n",
        "        \n",
        "        batch_size = attention_mask.shape[0]\n",
        "        device = attention_mask.device\n",
        "        dtype = attention_mask.dtype\n",
        "        \n",
        "        prompt_mask = torch.ones(batch_size, self.prompt_size, dtype=dtype, device=device)\n",
        "        \n",
        "        pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else 0\n",
        "        prompt_input_ids = torch.full(\n",
        "            (batch_size, self.prompt_size), \n",
        "            pad_token_id, \n",
        "            dtype=input_ids.dtype, \n",
        "            device=device\n",
        "        )\n",
        "        \n",
        "        output['attention_mask'] = torch.cat([prompt_mask, attention_mask], dim=1)\n",
        "        output['input_ids'] = torch.cat([prompt_input_ids, input_ids], dim=1)\n",
        "        \n",
        "        return output\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3caf1f53-f4f2-4e23-b338-da9e2f11af6f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-08T13:52:48.967115Z",
          "iopub.status.busy": "2026-02-08T13:52:48.966668Z",
          "iopub.status.idle": "2026-02-08T13:52:48.985316Z",
          "shell.execute_reply": "2026-02-08T13:52:48.984636Z",
          "shell.execute_reply.started": "2026-02-08T13:52:48.967058Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Применяем Prompt Tuning и LoRA к модели\n",
        "\n",
        "prompt_size = 20\n",
        "lora_rank = 8\n",
        "\n",
        "model.encoder.embed_tokens = PromptTuningEmbedding(model.encoder.embed_tokens, prompt_size).to(device)\n",
        "pt_lora_model = add_lora(model, lora_rank=lora_rank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c27d90ee-81cf-42a0-992c-3809e7b19bb7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-08T13:52:48.986576Z",
          "iopub.status.busy": "2026-02-08T13:52:48.986229Z",
          "iopub.status.idle": "2026-02-08T13:52:48.996040Z",
          "shell.execute_reply": "2026-02-08T13:52:48.995309Z",
          "shell.execute_reply.started": "2026-02-08T13:52:48.986527Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Замораживаем базовые веса, обучаем только промпты и LoRA-адаптеры\n",
        "\n",
        "for name, param in pt_lora_model.named_parameters():\n",
        "    if 'learnable_prompts' in name or 'adapter_A' in name or 'adapter_B' in name:\n",
        "        continue\n",
        "    else:\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dcda13c-785b-4e4b-ba9e-85220036163f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-08T13:52:48.997481Z",
          "iopub.status.busy": "2026-02-08T13:52:48.997109Z",
          "iopub.status.idle": "2026-02-08T13:52:49.012699Z",
          "shell.execute_reply": "2026-02-08T13:52:49.012093Z",
          "shell.execute_reply.started": "2026-02-08T13:52:48.997443Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Считаем долю обучаемых параметров\n",
        "\n",
        "model_params = sum(p.numel() for p in pt_lora_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in pt_lora_model.parameters() if p.requires_grad)\n",
        "print(\n",
        "    f'All params: %s | Trainable params: %s | Trainable %%: %.4f' % \\\n",
        "    (model_params, trainable_params, round(trainable_params / model_params, 4))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e13164f8-46d8-47c6-94ae-84b09c1d2efc",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2026-02-09T09:27:49.414159Z",
          "iopub.status.idle": "2026-02-09T09:27:49.414461Z",
          "shell.execute_reply": "2026-02-09T09:27:49.414342Z",
          "shell.execute_reply.started": "2026-02-09T09:27:49.414311Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Создаем DataCollator для обработки батчей с промпт-токенами \n",
        "\n",
        "data_collator = PTDataCollator(\n",
        "    prompt_size, tokenizer, model=pt_lora_model, padding=True, label_pad_token_id=-100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f06c1a4-e6be-444b-b98d-40d8a6f7649c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-08T13:52:49.039514Z",
          "iopub.status.busy": "2026-02-08T13:52:49.039259Z",
          "iopub.status.idle": "2026-02-08T20:25:45.527686Z",
          "shell.execute_reply": "2026-02-08T20:25:45.526699Z",
          "shell.execute_reply.started": "2026-02-08T13:52:49.039486Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Настраиваем параметры обучения модели\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    eval_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    num_train_epochs=10,\n",
        "    output_dir=\"./results\",\n",
        "    logging_steps=5,\n",
        "    logging_strategy=\"steps\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rouge1\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=2,\n",
        "    report_to=[],\n",
        "    disable_tqdm=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    predict_with_generate=True,\n",
        "    dataloader_num_workers=0, \n",
        "    save_safetensors=False,\n",
        ")\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=pt_lora_model,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['test'],\n",
        "    args=training_args,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[ClearMLCallback()]\n",
        ")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Обучаем модель\n",
        "trainer.train()\n",
        "\n",
        "# Сохраняем лучшую модель\n",
        "best_model_path = trainer.state.best_model_checkpoint\n",
        "\n",
        "# Загружаем модель в ClearML\n",
        "if best_model_path:\n",
        "    task_lora_pt.upload_artifact(\n",
        "        name=\"best_model\",\n",
        "        artifact_object=best_model_path\n",
        "    )\n",
        "    print(f\"Модель сохранена в ClearML: {best_model_path}\")\n",
        "else:\n",
        "    print(\"Путь к лучшей модели не найден\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9975af9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:26:23.170948Z",
          "iopub.status.busy": "2026-02-09T09:26:23.170615Z",
          "iopub.status.idle": "2026-02-09T09:26:23.180188Z",
          "shell.execute_reply": "2026-02-09T09:26:23.179523Z",
          "shell.execute_reply.started": "2026-02-09T09:26:23.170915Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Функция для подготовки входного текста - объединяет инструкцию и контекст\n",
        "def prepare_input_texts(df_row):\n",
        "    instruction = str(df_row['instruction']) if pd.notna(df_row['instruction']) else ''\n",
        "    input_text = str(df_row['input']) if pd.notna(df_row['input']) else ''\n",
        "    \n",
        "    if input_text:\n",
        "        return f'{instruction}\\n\\n{input_text}'\n",
        "    else:\n",
        "        return instruction\n",
        "\n",
        "# Функция для токенизации входного текста с учетом промпт-токенов - добавляет фиктивные input_ids и расширяет attention_mask\n",
        "def prepare_inputs_with_prompts(tokenizer, texts, prompt_size, device, max_length=100):\n",
        "    encoded = tokenizer(\n",
        "        texts,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors='pt'\n",
        "    ).to(device)\n",
        "    \n",
        "    current_batch_size = encoded['input_ids'].shape[0]\n",
        "    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
        "    \n",
        "    prompt_input_ids = torch.full(\n",
        "        (current_batch_size, prompt_size),\n",
        "        pad_token_id,\n",
        "        dtype=encoded['input_ids'].dtype,\n",
        "        device=device\n",
        "    )\n",
        "    \n",
        "    extended_input_ids = torch.cat([prompt_input_ids, encoded['input_ids']], dim=1)\n",
        "    \n",
        "    prompt_mask = torch.ones(\n",
        "        current_batch_size,\n",
        "        prompt_size,\n",
        "        dtype=encoded['attention_mask'].dtype,\n",
        "        device=device\n",
        "    )\n",
        "    extended_attention_mask = torch.cat([prompt_mask, encoded['attention_mask']], dim=1)\n",
        "    \n",
        "    return extended_input_ids, extended_attention_mask\n",
        "\n",
        "# Функция для генерирации предсказаний\n",
        "def generate_predictions(\n",
        "    model, \n",
        "    tokenizer, \n",
        "    input_texts, \n",
        "    prompt_size, \n",
        "    device,\n",
        "    max_length=200,\n",
        "    num_beams=4,\n",
        "    temperature=0.2,\n",
        "    top_k=30,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        "    **generate_kwargs\n",
        "):\n",
        "    extended_input_ids, extended_attention_mask = prepare_inputs_with_prompts(\n",
        "        tokenizer, input_texts, prompt_size, device\n",
        "    )\n",
        "    \n",
        "    default_kwargs = {\n",
        "        'max_length': max_length,\n",
        "        'num_beams': num_beams,\n",
        "        'temperature': temperature,\n",
        "        'top_k': top_k,\n",
        "        'top_p': top_p,\n",
        "        'do_sample': do_sample,\n",
        "        'early_stopping': True,\n",
        "        'eos_token_id': tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id,\n",
        "        'pad_token_id': tokenizer.pad_token_id\n",
        "    }\n",
        "    default_kwargs.update(generate_kwargs)\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        extended_input_ids,\n",
        "        attention_mask=extended_attention_mask,\n",
        "        **default_kwargs\n",
        "    )\n",
        "    \n",
        "    predictions = tokenizer.batch_decode(\n",
        "        outputs,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=True\n",
        "    )\n",
        "    \n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9318dc0b-6346-46fa-9b94-4c1e478b3f60",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-08T20:25:45.529230Z",
          "iopub.status.busy": "2026-02-08T20:25:45.528836Z",
          "iopub.status.idle": "2026-02-08T21:05:33.194792Z",
          "shell.execute_reply": "2026-02-08T21:05:33.194218Z",
          "shell.execute_reply.started": "2026-02-08T20:25:45.529174Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Оцениваем модель на тестовой выборке\n",
        "\n",
        "test_data = pd.read_csv('/kaggle/input/ru_instruct_gpt4_test.tsv', sep='\\t')\n",
        "\n",
        "pt_lora_model.eval()\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "batch_size = 2\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, i in enumerate(tqdm(range(0, len(test_data), batch_size))):\n",
        "        batch = test_data.iloc[i:i+batch_size]\n",
        "        \n",
        "        input_texts = [prepare_input_texts(row) for _, row in batch.iterrows()]\n",
        "        batch_refs = [str(row['output']) if pd.notna(row['output']) else '' for _, row in batch.iterrows()]\n",
        "        \n",
        "        batch_predictions = generate_predictions(\n",
        "            pt_lora_model,\n",
        "            tokenizer,\n",
        "            input_texts,\n",
        "            prompt_size,\n",
        "            device,\n",
        "            max_length=200,\n",
        "            num_beams=4,\n",
        "            temperature=0.2,\n",
        "            top_k=30,\n",
        "            top_p=0.95,\n",
        "            do_sample=True\n",
        "        )\n",
        "        \n",
        "        predictions.extend(batch_predictions)\n",
        "        references.extend(batch_refs)\n",
        "        \n",
        "        if (batch_idx + 1) % 5 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "references_list = [[ref] for ref in references]\n",
        "\n",
        "bleu_result = bleu_metric.compute(predictions=predictions, references=references_list)\n",
        "rouge_result = rouge_metric.compute(predictions=predictions, references=references)\n",
        "meteor_result = meteor_metric.compute(predictions=predictions, references=references_list)\n",
        "\n",
        "print(f\"BLEU: {bleu_result['bleu']:.4f}\")\n",
        "print(f\"ROUGE-1: {rouge_result['rouge1']:.4f}\")\n",
        "print(f\"METEOR: {meteor_result['meteor']:.4f}\")\n",
        "\n",
        "task_lora_pt.connect({\n",
        "    \"test_bleu\": bleu_result['bleu'],\n",
        "    \"test_rouge1\": rouge_result['rouge1'],\n",
        "    \"test_meteor\": meteor_result['meteor']\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18bc3dfe-0fbc-425b-a9b4-ca72ab8c3017",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-08T21:05:33.198233Z",
          "iopub.status.busy": "2026-02-08T21:05:33.197653Z",
          "iopub.status.idle": "2026-02-08T21:05:35.022193Z",
          "shell.execute_reply": "2026-02-08T21:05:35.021545Z",
          "shell.execute_reply.started": "2026-02-08T21:05:33.198202Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Проверяем примеры генерации после дообучения\n",
        "\n",
        "lm_text='Придумайте название для книги на основе следующего описания сюжета.\\n\\nМолодой парень находит портал в параллельный мир, где его жизнь намного хуже, чем она была до этого.'\n",
        "\n",
        "predictions = generate_predictions(\n",
        "    pt_lora_model,\n",
        "    tokenizer,\n",
        "    [lm_text],\n",
        "    prompt_size,\n",
        "    device,\n",
        "    max_length=200,\n",
        "    num_beams=4,\n",
        "    temperature=0.2,\n",
        "    top_k=30,\n",
        "    top_p=0.95,\n",
        "    do_sample=True\n",
        "        )\n",
        "\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9c0639d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:32:19.658105Z",
          "iopub.status.busy": "2026-02-09T09:32:19.657354Z",
          "iopub.status.idle": "2026-02-09T09:32:20.116104Z",
          "shell.execute_reply": "2026-02-09T09:32:20.115474Z",
          "shell.execute_reply.started": "2026-02-09T09:32:19.658069Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "lm_text='Придумай мотивирующую цитату.\\n\\nСпорт,усилия,результат.'\n",
        "\n",
        "predictions = generate_predictions(\n",
        "    pt_lora_model,\n",
        "    tokenizer,\n",
        "    [lm_text],\n",
        "    prompt_size,\n",
        "    device,\n",
        "    max_length=200,\n",
        "    num_beams=2,\n",
        "    temperature=0.2,\n",
        "    top_k=30,\n",
        "    top_p=0.95,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69c32cd7-d7ba-420b-affa-2bc131dc55a5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T10:13:36.630968Z",
          "iopub.status.busy": "2026-02-09T10:13:36.630268Z",
          "iopub.status.idle": "2026-02-09T10:13:38.726568Z",
          "shell.execute_reply": "2026-02-09T10:13:38.725940Z",
          "shell.execute_reply.started": "2026-02-09T10:13:36.630935Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "lm_text='Используя указанную структуру, создай рассказ.\\n\\nЛето, школьница и учитель, встреча'\n",
        "\n",
        "predictions = generate_predictions(\n",
        "    pt_lora_model,\n",
        "    tokenizer,\n",
        "    [lm_text],\n",
        "    prompt_size,\n",
        "    device,\n",
        "    max_length=200,\n",
        "    num_beams=2,\n",
        "    temperature=0.2,\n",
        "    top_k=30,\n",
        "    top_p=0.95,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b54fac43-f29d-485c-a07e-c061d317037c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:38:39.761607Z",
          "iopub.status.busy": "2026-02-09T09:38:39.760990Z",
          "iopub.status.idle": "2026-02-09T09:38:43.470253Z",
          "shell.execute_reply": "2026-02-09T09:38:43.469482Z",
          "shell.execute_reply.started": "2026-02-09T09:38:39.761570Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "lm_text='Расскажи историю про медведя'\n",
        "\n",
        "predictions = generate_predictions(\n",
        "    pt_lora_model,\n",
        "    tokenizer,\n",
        "    [lm_text],\n",
        "    prompt_size,\n",
        "    device,\n",
        "    max_length=200,\n",
        "    num_beams=4,\n",
        "    temperature=0.8,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16cad85f-4e2d-4d30-81bc-20eb626e4c52",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T09:55:16.753808Z",
          "iopub.status.busy": "2026-02-09T09:55:16.753502Z",
          "iopub.status.idle": "2026-02-09T09:55:19.065755Z",
          "shell.execute_reply": "2026-02-09T09:55:19.064975Z",
          "shell.execute_reply.started": "2026-02-09T09:55:16.753779Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "lm_text='Расскажи историю о девочке. В истории должны быть:игры, подружка, счастливая история'\n",
        "\n",
        "predictions = generate_predictions(\n",
        "    pt_lora_model,\n",
        "    tokenizer,\n",
        "    [lm_text],\n",
        "    prompt_size,\n",
        "    device,\n",
        "    max_length=200,\n",
        "    num_beams=1,\n",
        "    temperature=0.9,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b7ca85-8db7-43db-8e9b-e4ebddb5d498",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T10:07:22.875942Z",
          "iopub.status.busy": "2026-02-09T10:07:22.875104Z",
          "iopub.status.idle": "2026-02-09T10:07:25.043584Z",
          "shell.execute_reply": "2026-02-09T10:07:25.042932Z",
          "shell.execute_reply.started": "2026-02-09T10:07:22.875905Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "lm_text='Что означает слово \"землетрясение\".'\n",
        "\n",
        "predictions = generate_predictions(\n",
        "    pt_lora_model,\n",
        "    tokenizer,\n",
        "    [lm_text],\n",
        "    prompt_size,\n",
        "    device,\n",
        "    max_length=200,\n",
        "    num_beams=1,\n",
        "    temperature=0.9,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96f6ea17-c35b-4399-b1cc-fec5bdeceaf0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-09T10:10:56.055521Z",
          "iopub.status.busy": "2026-02-09T10:10:56.054975Z",
          "iopub.status.idle": "2026-02-09T10:10:56.873029Z",
          "shell.execute_reply": "2026-02-09T10:10:56.872220Z",
          "shell.execute_reply.started": "2026-02-09T10:10:56.055485Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "lm_text='Придумай мотивирующую цитату, которая поднимет настроение для учебы.'\n",
        "\n",
        "predictions = generate_predictions(\n",
        "    pt_lora_model,\n",
        "    tokenizer,\n",
        "    [lm_text],\n",
        "    prompt_size,\n",
        "    device,\n",
        "    max_length=200,\n",
        "    num_beams=1,\n",
        "    temperature=0.9,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec2fed14-953a-417c-a21e-d5fa69e4db00",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-08T21:05:38.943971Z",
          "iopub.status.busy": "2026-02-08T21:05:38.943658Z",
          "iopub.status.idle": "2026-02-08T21:05:50.374901Z",
          "shell.execute_reply": "2026-02-08T21:05:50.374086Z",
          "shell.execute_reply.started": "2026-02-08T21:05:38.943938Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "task_lora_pt.close()"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 9396645,
          "sourceId": 14707822,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31260,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
